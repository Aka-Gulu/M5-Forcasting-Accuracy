#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder


# In[2]:


sales = pd.read_csv('sales_train_evaluation.csv')
calendar = pd.read_csv('calendar.csv')
price = pd.read_csv('sell_prices.csv')


# In[3]:


def reduce_mem_usage(df):
    """ iterate through all the columns of a dataframe and modify the data type
        to reduce memory usage.
    """
    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))

    for col in df.columns:
        col_type = df[col].dtype

        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))

    return df


# In[4]:


sales = reduce_mem_usage(sales)
calendar = reduce_mem_usage(calendar)
price = reduce_mem_usage(price)


# In[5]:


df = pd.melt(sales, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='sold').dropna()
df = pd.merge(df, calendar, on='d', how='left')
df = pd.merge(df, price, on=['store_id','item_id','wm_yr_wk'], how='left')
display(df.head())
print(df.info())


# In[6]:


df.drop(["wm_yr_wk"],axis=1,inplace=True)


# In[7]:


df.drop(["weekday"],axis=1,inplace=True)


# In[8]:


df.d = df['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)
print(df.info())


# In[9]:


cols = df.dtypes.index.tolist()
types = df.dtypes.values.tolist()
for i,type in enumerate(types):
    if type.name == 'category':
        df[cols[i]] = df[cols[i]].cat.codes

#lags = [7,14,28,56]

lags = [29,30,31,32,33,34,35,40,55,60,65,180]
for lag in lags:
    df['sold_lag_'+str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],
                                          as_index=False)['sold'].shift(lag+28).astype(np.float16)

sold_lag_cols = ['sold_lag_29'] 

for win in [7, 28]:
    df['rmean_29_{}'.format(win)] = df.groupby(['id', 
                                                'item_id', 
                                                'dept_id', 
                                                'cat_id',
                                                'store_id', 
                                                'state_id'])['sold_lag_29'].transform(lambda x: x.rolling(window=win).mean()).astype(np.float16)

# In[10]:


df['item_sold_avg'] = df.groupby('item_id')['sold'].transform('mean').astype(np.float16)    
df['state_sold_avg'] = df.groupby('state_id')['sold'].transform('mean').astype(np.float16)
df['store_sold_avg'] = df.groupby('store_id')['sold'].transform('mean').astype(np.float16)
df['cat_sold_avg'] = df.groupby('cat_id')['sold'].transform('mean').astype(np.float16)
df['dept_sold_avg'] = df.groupby('dept_id')['sold'].transform('mean').astype(np.float16)
df['cat_dept_sold_avg'] = df.groupby(['cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)
df['store_item_sold_avg'] = df.groupby(['store_id','item_id'])['sold'].transform('mean').astype(np.float16)
df['cat_item_sold_avg'] = df.groupby(['cat_id','item_id'])['sold'].transform('mean').astype(np.float16)
df['dept_item_sold_avg'] = df.groupby(['dept_id','item_id'])['sold'].transform('mean').astype(np.float16)
df['state_store_sold_avg'] = df.groupby(['state_id','store_id'])['sold'].transform('mean').astype(np.float16)
df['state_store_cat_sold_avg'] = df.groupby(['state_id','store_id','cat_id'])['sold'].transform('mean').astype(np.float16)
df['store_cat_dept_sold_avg'] = df.groupby(['store_id','cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)


# In[11]:


df['price_lag'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sell_price'].shift(7).astype(np.float16)
df['price-diff']=df['price_lag']-df['sell_price']
df.drop(['price_lag'], axis=1, inplace=True)

df['expanding_price_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sell_price'].transform(lambda x: x.expanding(2).mean()).astype(np.float16)
df['diff_moving_mean']=df['expanding_price_mean']-df['sell_price']
df.drop(['expanding_price_mean'], axis=1, inplace=True)

df['price-diff']=df['price-diff'].astype(np.float16)


# In[12]:


#df['wm_yr_wk_linear']=LabelEncoder().fit_transform(df['wm_yr_wk'].values).astype(np.int16)
#df.drop(['wm_yr_wk'], axis=1, inplace=True)


#df.drop(['wday'], axis=1, inplace=True)S
#df['decimal']=df['decimal'].astype(np.int8)
#df['year']=LabelEncoder().fit_transform(df['year']).astype(np.int8)

df['daily_avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','d'])['sell_price'].transform('mean').astype(np.float16)
df['avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sell_price'].transform('mean').astype(np.float16)
df['selling_trend'] = (df['daily_avg_sold'] - df['avg_sold']).astype(np.float16)
df.drop(['daily_avg_sold','avg_sold'],axis=1,inplace=True)


#df['price_max'] = df.groupby(['store_id','item_id'])['sell_price'].transform('max')
#df['price_min'] = df.groupby(['store_id','item_id'])['sell_price'].transform('min')
#df['price_std'] = df.groupby(['store_id','item_id'])['sell_price'].transform('std')
df['price_mean'] = df.groupby(['store_id','item_id'])['sell_price'].transform('mean')
#df['price_norm'] = df['sell_price']/df['price_max']
df['price_momentum'] = df['sell_price']/df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))
df['price_momentum_m'] = df['sell_price']/df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')
df['price_momentum_y'] = df['sell_price']/df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')


# In[13]:


print(df.info())


# In[14]:


cat_feats = ['item_id', 'dept_id','store_id', 'cat_id']
for cc in cat_feats:
    le = LabelEncoder()
    df[cc] = le.fit_transform(df[cc])


# In[15]:


print(df.info())


# In[ ]:


#X_train, y_train = df[df['d']<1914].drop('sold',axis=1), df[df['d']<1914]['sold']
#X_valid, y_valid = df[(df['d']>=1914) & (df['d']<1942)].drop('sold',axis=1), df[(df['d']>=1914) & (df['d']<1942)]['sold']


# In[ ]:


#X_train.drop(['id', 'snap_CA', 'year', 'date'], axis = 1, inplace = True)
#X_valid.drop(['id', 'snap_CA', 'year', 'date'], axis = 1, inplace = True)


# In[ ]:


#store_counts = df['store_id'].value_counts()
#print(store_counts)
#state_counts = df['state_id'].value_counts()
#print(state_counts)


# In[16]:


df['date'] = pd.to_datetime(df["date"])

date_features = {
    "wday": "weekday",
    # Use "isocalendar().week" instead of "weekofyear"
    "week": ("isocalendar", "week"),
    "month": "month",
    "quarter": "quarter",
    "year": "year",
    "mday": "day",
}

for date_feat_name, date_feat_func in date_features.items():
    if date_feat_name in df.columns:
        df[date_feat_name] = df[date_feat_name].astype("int16")
    else:
        # Handle the case for week differently
        if date_feat_name == "week":
            df[date_feat_name] = df["date"].apply(lambda x: x.isocalendar()[1]).astype("int16")
        else:
            df[date_feat_name] = getattr(df["date"].dt, date_feat_func).astype("int16")
    

print(df.info())


# In[17]:


df.drop(['id', 'wday','year', 'date'], axis = 1, inplace = True)


# In[18]:


d_state_id = [0,1,2]
d_store_id = [0,1,2,3,4,5,6,7,8,9]


# In[19]:


train = df[df['d'] < 1914]
valid = df[(df['d'] >= 1914) & (df['d'] < 1942)]


# In[20]:


df = df[df['d']>= 180]
df.to_pickle('data.pkl')

train = train[train['d']>=180]
train.to_pickle('train.pkl')

valid = valid[valid['d']>=180]
valid.to_pickle('valid.pkl')

del df, sales, price, calendar, train, valid


# In[21]:


import gc
gc.collect()


# In[ ]:


#valid_preds_csv = pd.Series(index=df.index)
#eval_preds = pd.Series(index=df.index)

#for store in d_store_id:
#    store_df = data[data['store_id'] == store]
#    X_train = store_df[store_df['d'] < 1914].drop('sold', axis=1)
#    y_train = store_df[store_df['d'] < 1914]['sold']
 #   X_valid_csv = store_df[(store_df['d'] >= 1914) & (store_df['d'] < 1942)].drop('sold', axis=1)
 #   y_valid_csv = store_df[(store_df['d'] >= 1914) & (store_df['d'] < 1942)]['sold']
 #   
  #  print('shape:', store, store_df.shape)
  #  print('X_train shape:', X_train.shape)
  #  print('y_train shape:', y_train.shape)
  #  print('X_valid_csv shape:', X_valid_csv.shape)
  #  print('y_valid_csv shape:', y_valid_csv.shape)


# In[39]:


from lightgbm import LGBMRegressor
import optuna
import sklearn.datasets
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import lightgbm as lgb
from sklearn.datasets import make_regression
import joblib
from lightgbm import LGBMRegressor, early_stopping



data = pd.read_pickle('data.pkl')
train = pd.read_pickle('train.pkl')
valid = pd.read_pickle('valid.pkl')

valid_preds_csv = pd.Series(index=valid.index)

for state in d_state_id:

    X_train = train[train['state_id'] == state].drop(['sold'], axis=1)  
    y_train = train[train['state_id'] == state]['sold']

    X_valid_csv = valid[valid['state_id'] == state].drop(['sold'], axis=1)
    y_valid_csv = valid[valid['state_id'] == state]['sold']
    
    model = LGBMRegressor(
        objective = 'tweedie', 
        learning_rate=0.05,
        subsample=0.6,
        feature_fraction=0.6,
        num_iterations=1200,
        max_bin=350,
        num_leaves=100,
        lambda_l2=0.003,
        max_depth=200,
        min_data_in_leaf=80,
        force_row_wise=True
    )
    print('*****Prediction for State: {}*****'.format(state))
    early_stopping_callback = early_stopping(stopping_rounds=50, verbose=100)
    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid_csv, y_valid_csv)],
              eval_metric='rmse', callbacks=[early_stopping_callback])
    valid_preds_csv[X_valid_csv.index] = model.predict(X_valid_csv)


# In[40]:
y_train.plot.density()

import lightgbm as lgb
import matplotlib.pyplot as plt

# Assuming `model` is your trained LGBMRegressor model
lgb.plot_importance(model, max_num_features=20, importance_type='split')
plt.title("Feature Importance")
plt.show()


# In[45]:


gc.collect()


# In[44]:


del data, train, valid,  X_train,y_train


# In[ ]:

from lightgbm import LGBMRegressor
import optuna
import sklearn.datasets
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import lightgbm as lgb
from sklearn.datasets import make_regression
import joblib
from lightgbm import LGBMRegressor, early_stopping



data = pd.read_pickle('data.pkl')
train = pd.read_pickle('train.pkl')
valid = pd.read_pickle('valid.pkl')

valid_preds_csv = pd.Series(index=valid.index)

for state in d_state_id:

    X_train = train[train['state_id'] == state].drop(['sold'], axis=1)  
    y_train = train[train['state_id'] == state]['sold']

    X_valid_csv = valid[valid['state_id'] == state].drop(['sold'], axis=1)
    y_valid_csv = valid[valid['state_id'] == state]['sold']
    
    model = LGBMRegressor(
        objective = 'tweedie', 
        learning_rate=0.075,
        tweedie_variance_power=1.5,
        min_child_samples=10,
        #subsample=0.6,
        #feature_fraction=0.6,
        num_iterations=500,
        #max_bin=350,
        num_leaves=128,
        lambda_l2=0.1,
       # max_depth=8
        #force_row_wise=True
        sub_feature = 0.8,
        sub_row = 0.8,
        bagging_freq = 1,
        
    )
    print('*****Prediction for State: {}*****'.format(state))
    early_stopping_callback = early_stopping(stopping_rounds=50, verbose=100)
    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid_csv, y_valid_csv)],
              eval_metric='rmse', callbacks=[early_stopping_callback])
    valid_preds_csv[X_valid_csv.index] = model.predict(X_valid_csv)




# In[22]:


from lightgbm import LGBMRegressor
import optuna
import sklearn.datasets
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import lightgbm as lgb
from sklearn.datasets import make_regression
import joblib
from lightgbm import LGBMRegressor, early_stopping



data = pd.read_pickle('data.pkl')
train = pd.read_pickle('train.pkl')
valid = pd.read_pickle('valid.pkl')

valid_preds_csv = pd.Series(index=valid.index)

for state in d_state_id:

    X_train = train[train['state_id'] == state].drop(['sold','d'], axis=1)  
    y_train = train[train['state_id'] == state]['sold']

    X_valid_csv = valid[valid['state_id'] == state].drop(['sold','d'], axis=1)
    y_valid_csv = valid[valid['state_id'] == state]['sold']
    
    model = LGBMRegressor(
        objective = 'tweedie',
        learning_rate=0.05,
        subsample=0.6,
        feature_fraction=0.6,
        num_iterations=1200,
        max_bin=350,
        num_leaves=100,
        lambda_l2=0.003,
        max_depth=200,
        min_data_in_leaf=80,
        force_row_wise=True
    )
    print('*****Prediction for Store: {}*****'.format(state))
    early_stopping_callback = early_stopping(stopping_rounds=50, verbose=100)
    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid_csv, y_valid_csv)],
              eval_metric='rmse', callbacks=[early_stopping_callback])
    valid_preds_csv[X_valid_csv.index] = model.predict(X_valid_csv)


# In[23]:


import lightgbm as lgb
import matplotlib.pyplot as plt

# Assuming `model` is your trained LGBMRegressor model
lgb.plot_importance(model, max_num_features=20, importance_type='split')
plt.title("Feature Importance")
plt.show()


# In[ ]:


from lightgbm import LGBMRegressor
import optuna
import sklearn.datasets
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import lightgbm as lgb
from sklearn.datasets import make_regression
import joblib
from lightgbm import LGBMRegressor, early_stopping



seed = 123

def objectives(trial):
    params = {
        'objective': 'tweedie',
        'tweedie_variance_power': 1.5,  # Typical value for Tweedie regression, adjust as needed
        'metric': 'rmse',
        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.1),
        'num_leaves': trial.suggest_int('num_leaves', 63, 255),
        'max_depth': trial.suggest_int('max_depth', 4, 8),
        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),
    }
    
    early_stopping_callback = early_stopping(stopping_rounds=50, first_metric_only=False, verbose=False, min_delta=0.0)
    
    model = LGBMRegressor(random_state=seed, **params)
    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], callbacks=[early_stopping_callback])
    preds = model.predict(X_valid)
    score = np.sqrt(mean_squared_error(y_valid, preds))
    
    return score

# Optuna optimization
opt = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=seed))
opt.optimize(objectives, n_trials=50)

# Best parameters
best_params = opt.best_trial.params
best_params['objective'] = 'tweedie'
best_params['metric'] = 'rmse'
best_params['tweedie_variance_power'] = 1.5  # Adjust as needed

print('Best parameters:', best_params)


# In[ ]:


from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error

# Define your dataset and features here: X_train, y_train, X_valid_csv, y_valid_csv, X_test

model = LGBMRegressor(
    learning_rate=0.05,
    subsample=0.6,
    feature_fraction=0.6,
    num_iterations=1200,
    max_bin=350,
    num_leaves=100,
    lambda_l2=0.003,
    max_depth=200,
    min_data_in_leaf=80,
    force_row_wise=True
)

print('*****Training RMSE*****')
early_stopping_callback = early_stopping(stopping_rounds=50, verbose=False)
model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)],
          eval_metric='rmse', callbacks=[early_stopping_callback])
train_preds = model.predict(X_train)
train_rmse = mean_squared_error(y_train, train_preds, squared=False)
print('Training RMSE:', train_rmse)

print('*****Validation RMSE*****')
valid_preds = model.predict(X_valid)
valid_rmse = mean_squared_error(y_valid, valid_preds, squared=False)
print('Validation RMSE:', valid_rmse)


# In[ ]:


print(df.info())
value_counts = df['store_id'].value_counts()
print(value_counts)


# In[ ]:


d_store_id = [0,1,2,3]
data = df
valid_preds_csv = pd.Series(index=df.index)
eval_preds = pd.Series(index=df.index)

for store in d_store_id:
    store_df = data[data['store_id'] == store]
    
    X_train = store_df[store_df['d'] < 1914].drop('sold', axis=1)
    y_train = store_df[store_df['d'] < 1914]['sold']
    X_valid_csv = store_df[(store_df['d'] >= 1914) & (store_df['d'] < 1942)].drop('sold', axis=1)
    y_valid_csv = store_df[(store_df['d'] >= 1914) & (store_df['d'] < 1942)]['sold']
    
    model = LGBMRegressor(
        learning_rate=0.05,
        subsample=0.6,
        feature_fraction=0.6,
        num_iterations=1200,
        max_bin=350,
        num_leaves=100,
        lambda_l2=0.003,
        max_depth=200,
        min_data_in_leaf=80,
        force_row_wise=True
    )
    print('*****Prediction for Store: {}*****'.format(store))
    early_stopping_callback = early_stopping(stopping_rounds=50, verbose=100)
    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid_csv, y_valid_csv)],
              eval_metric='rmse', callbacks=[early_stopping_callback])
    valid_preds_csv[X_valid_csv.index] = model.predict(X_valid_csv)


# In[ ]:




